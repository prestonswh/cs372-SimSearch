{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M1aBpWx5kVEn"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i45doXd2SQo2",
        "outputId": "66285aab-8823-4762-bcd5-8f0808952f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRxDzGNaSVSi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from itertools import islice # for slicing and dicing JSON records\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvJIWjvHXHT3"
      },
      "outputs": [],
      "source": [
        "def get_data(json_filename = 'arxiv-metadata-oai-snapshot.json', data_root = '/content/drive/MyDrive/카이스트/23봄/CS372/project'):\n",
        "  with open(data_root + '/' + 'dataset' + '/' + json_filename, \"rb\") as f:\n",
        "    for line in f:\n",
        "      yield line\n",
        "\n",
        "data_gen = get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGZPATgBb7-J"
      },
      "outputs": [],
      "source": [
        "def get_records(data_gen, chunksize=500):\n",
        "    return [json.loads(record) for record in islice(data_gen, chunksize)]\n",
        "\n",
        "records_per_chunk = 250000\n",
        "data_records = get_records(data_gen, records_per_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwgqMe2aczT0"
      },
      "outputs": [],
      "source": [
        "def split_records(data_records, num_profiles=100, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    np.random.shuffle(data_records)\n",
        "    train_records, test_records = data_records[:num_profiles], data_records[num_profiles:]\n",
        "    return train_records, test_records\n",
        "\n",
        "# Splitting the fetched records into train and test records\n",
        "train_records, test_records = split_records(data_records, num_profiles=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDLDrnsOc-0K"
      },
      "outputs": [],
      "source": [
        "# Utility method to generate dataframe from list of dictionaries\n",
        "def get_dataframe(list_of_dicts, columns=None):\n",
        "    data = pd.DataFrame(list_of_dicts)\n",
        "    if columns:\n",
        "        data.columns = columns\n",
        "    return data\n",
        "\n",
        "# Generating dataframes for train and test records\n",
        "train_df = get_dataframe(train_records)\n",
        "test_df = get_dataframe(test_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYLhvsaNdB_6"
      },
      "outputs": [],
      "source": [
        "# Utility method to filter out certain features which are of use\n",
        "def filter_features(data, features):\n",
        "    return data[features]\n",
        "\n",
        "# Filtering the test dataframes for features we selected\n",
        "features = ['title', 'categories', 'abstract', 'update_date']\n",
        "train_df = filter_features(train_df, features)\n",
        "test_df = filter_features(test_df, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XInQTd2dENK"
      },
      "outputs": [],
      "source": [
        "# define the corpus to pull from\n",
        "train_corpus = train_df['abstract'].head(10000)\n",
        "test_corpus = test_df['abstract'].head(10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-w5oZt5LnCOY",
        "outputId": "41268348-dc6d-49ed-eaf0-4d8a4bfefeed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-07f49099-c5ca-45c6-996c-c56754509d69\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>categories</th>\n",
              "      <th>abstract</th>\n",
              "      <th>update_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Probing FSR star cluster candidates in bulge/d...</td>\n",
              "      <td>astro-ph</td>\n",
              "      <td>We analyse 20 star cluster candidates projec...</td>\n",
              "      <td>2012-04-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Further Development of the Tetron Model</td>\n",
              "      <td>hep-ph</td>\n",
              "      <td>After a prologue which clarifies some issues...</td>\n",
              "      <td>2014-11-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Characterisation of a three-dimensional Browni...</td>\n",
              "      <td>physics.atom-ph</td>\n",
              "      <td>We present here a detailed study of the beha...</td>\n",
              "      <td>2007-09-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Nonmagnetic Impurity Resonances as a Signature...</td>\n",
              "      <td>cond-mat.supr-con</td>\n",
              "      <td>The low energy band structure of the FeAs ba...</td>\n",
              "      <td>2013-05-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Analysis of optical properties of strained sem...</td>\n",
              "      <td>cond-mat.mes-hall</td>\n",
              "      <td>Using multiband k*p theory we study the size...</td>\n",
              "      <td>2010-02-11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07f49099-c5ca-45c6-996c-c56754509d69')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07f49099-c5ca-45c6-996c-c56754509d69 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07f49099-c5ca-45c6-996c-c56754509d69');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               title         categories  \\\n",
              "0  Probing FSR star cluster candidates in bulge/d...           astro-ph   \n",
              "1            Further Development of the Tetron Model             hep-ph   \n",
              "2  Characterisation of a three-dimensional Browni...    physics.atom-ph   \n",
              "3  Nonmagnetic Impurity Resonances as a Signature...  cond-mat.supr-con   \n",
              "4  Analysis of optical properties of strained sem...  cond-mat.mes-hall   \n",
              "\n",
              "                                            abstract update_date  \n",
              "0    We analyse 20 star cluster candidates projec...  2012-04-02  \n",
              "1    After a prologue which clarifies some issues...  2014-11-18  \n",
              "2    We present here a detailed study of the beha...  2007-09-18  \n",
              "3    The low energy band structure of the FeAs ba...  2013-05-29  \n",
              "4    Using multiband k*p theory we study the size...  2010-02-11  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8NcKWXVadGrX",
        "outputId": "ed5e8aef-3563-471e-8130-ba97b2f60bcc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b2752170-5acc-4f88-8c76-3d1c209d3df9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>categories</th>\n",
              "      <th>abstract</th>\n",
              "      <th>update_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Strichartz Estimates for Water Waves</td>\n",
              "      <td>math.AP</td>\n",
              "      <td>In this paper we investigate the dispersive ...</td>\n",
              "      <td>2010-02-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Analysis of Two Eclipsing Hot Subdwarf Binarie...</td>\n",
              "      <td>astro-ph.EP astro-ph.SR</td>\n",
              "      <td>The formation of hot subdwarf stars (sdBs), ...</td>\n",
              "      <td>2015-03-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Bayesian reassessment of nearest-neighbour c...</td>\n",
              "      <td>stat.CO math.ST stat.TH</td>\n",
              "      <td>The k-nearest-neighbour procedure is a well-...</td>\n",
              "      <td>2008-02-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Clusters, Graphs, and Networks for Analysing I...</td>\n",
              "      <td>cs.AI cs.LG</td>\n",
              "      <td>The proposal is to use clusters, graphs and ...</td>\n",
              "      <td>2007-07-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>On the Capacity of the Heat Channel, Waterfill...</td>\n",
              "      <td>cs.IT math.IT</td>\n",
              "      <td>The heat channel is defined by a linear time...</td>\n",
              "      <td>2014-01-28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2752170-5acc-4f88-8c76-3d1c209d3df9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2752170-5acc-4f88-8c76-3d1c209d3df9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2752170-5acc-4f88-8c76-3d1c209d3df9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               title               categories  \\\n",
              "0               Strichartz Estimates for Water Waves                  math.AP   \n",
              "1  Analysis of Two Eclipsing Hot Subdwarf Binarie...  astro-ph.EP astro-ph.SR   \n",
              "2  A Bayesian reassessment of nearest-neighbour c...  stat.CO math.ST stat.TH   \n",
              "3  Clusters, Graphs, and Networks for Analysing I...              cs.AI cs.LG   \n",
              "4  On the Capacity of the Heat Channel, Waterfill...            cs.IT math.IT   \n",
              "\n",
              "                                            abstract update_date  \n",
              "0    In this paper we investigate the dispersive ...  2010-02-02  \n",
              "1    The formation of hot subdwarf stars (sdBs), ...  2015-03-18  \n",
              "2    The k-nearest-neighbour procedure is a well-...  2008-02-12  \n",
              "3    The proposal is to use clusters, graphs and ...  2007-07-11  \n",
              "4    The heat channel is defined by a linear time...  2014-01-28  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKDa6YDgsF9Y"
      },
      "outputs": [],
      "source": [
        "# train_df.to_csv('/content/drive/MyDrive/cs372/project/dataset/train_df.csv', index = False)\n",
        "# test_df.to_csv('/content/drive/MyDrive/cs372/project/dataset/test_df.csv', index = False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P4SVBDq6USxq"
      },
      "source": [
        "## Removing unnecessary words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbVZHT-8X2ZS",
        "outputId": "dd9eb5ed-55b0-4dba-f74b-a5632b720ebb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('book')\n",
        "from nltk.book import *\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_INQVwqFUkKH"
      },
      "outputs": [],
      "source": [
        "def tokenize_POS(paragraph):\n",
        "  words = word_tokenize(paragraph)\n",
        "  tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "  # Remove not important types of words\n",
        "  excluded_tags = ['CC', 'DT', 'IN', 'TO', 'PRP', 'PRP$', 'MD', 'WP', 'WP$', 'WRB']\n",
        "  filtered_words = [word for word, pos in tagged_words if pos not in excluded_tags]\n",
        "\n",
        "  return ' '.join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYX4lXikbRJv"
      },
      "outputs": [],
      "source": [
        "# define the corpus to pull from\n",
        "train_corpus = train_df['abstract'].head(10000)\n",
        "test_corpus = test_df['abstract'].head(10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "iVuQwcC1YhY9",
        "outputId": "62067e8a-2692-4105-b49b-8e8c1d8f01e7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  We analyse 20 star cluster candidates projected mostly in the bulge direction\\n($|\\\\ell|<60^\\\\circ$). The sample contains all candidates in that sector\\nclassified by \\\\citet{FSRcat} with quality flags denoting high probability of\\nbeing star clusters. Bulge contamination in the colour-magnitude diagrams\\n(CMDs) is in general important, while at lower Galactic latitudes disk stars\\ncontribute as well. Properties of the candidates are investigated with 2MASS\\nCMDs and stellar radial density profiles (RDPs) built with field star\\ndecontaminated photometry. To uncover the nature of the structures we\\ndecontaminate the CMDs from field stars using tools that we previously\\ndeveloped to deal with objects in dense fields. We confirm in all cases\\nexcesses in the RDPs with respect to the background level, as expected from the\\nmethod the candidates were originally selected. CMDs and RDPs taken together\\nrevealed 6 open clusters, 5 uncertain cases that require deeper observations,\\nwhile 9 objects are possibly field density fluctuations.\\n'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_corpus[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSOLX_c6XsXt"
      },
      "outputs": [],
      "source": [
        "train_corpus = pd.Series([tokenize_POS(abstract) for abstract in train_corpus])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "hH4qxNgee7Wq",
        "outputId": "e1fe5d0e-2fad-4698-95eb-4d37c6b4d588"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'analyse 20 star cluster candidates projected mostly bulge direction ( $ |\\\\ell| < 60^\\\\circ $ ) . sample contains candidates sector classified \\\\citet { FSRcat } quality flags denoting high probability being star clusters . Bulge contamination colour-magnitude diagrams ( CMDs ) is general important , lower Galactic latitudes disk stars contribute as well . Properties candidates are investigated 2MASS CMDs stellar radial density profiles ( RDPs ) built field star decontaminated photometry . uncover nature structures decontaminate CMDs field stars using tools previously developed deal objects dense fields . confirm cases excesses RDPs respect background level , expected method candidates were originally selected . CMDs RDPs taken together revealed 6 open clusters , 5 uncertain cases that require deeper observations , 9 objects are possibly field density fluctuations .'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_corpus[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AbGe1M1deQp"
      },
      "outputs": [],
      "source": [
        "train_df_doc = train_df\n",
        "\n",
        "train_df_doc['abstract'] = train_df_doc['abstract'].apply(tokenize_POS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FHCuktPM8jj_"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dicpr5VL9CD9"
      },
      "outputs": [],
      "source": [
        "import time # for getting the runtime of cells\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # for building word representations\n",
        "from sklearn.metrics.pairwise import cosine_similarity # for getting similarity metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeotfSV897z5"
      },
      "outputs": [],
      "source": [
        "def get_recommendations_TFIDF(abstract):\n",
        "  abstract = tokenize_POS(abstract)\n",
        "  corpus = pd.concat([train_corpus, pd.Series(abstract)], ignore_index=True)\n",
        "\n",
        "  # Initialize an instance of tf-idf Vectorizer\n",
        "  tfidf_vectorizer = TfidfVectorizer()\n",
        "  # Generate the tf-idf vectors for the corpus\n",
        "  tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "  # compute and print the cosine similarity matrix\n",
        "  cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "  # Get the pairwise similarity scores\n",
        "  sim_scores = list(enumerate(cosine_sim[-1]))\n",
        "  sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  # Check if the first result is the input abstract\n",
        "  if corpus[int(sim_scores[0][0])].split() == abstract.split() and corpus[int(sim_scores[1][0])].split() == abstract.split():\n",
        "    print(corpus[int(sim_scores[0][0])].split() == abstract.split())\n",
        "    print(corpus[int(sim_scores[1][0])].split() == abstract.split())\n",
        "    paper_indices = int(sim_scores[2][0])\n",
        "    similarity = \"{:.2f}%\".format(sim_scores[2][1] * 100)  # Format similarity as a string with two decimal places and a percentage sign\n",
        "  elif sim_scores[0][0] == 500:\n",
        "    paper_indices = int(sim_scores[1][0])\n",
        "    similarity = \"{:.2f}%\".format(sim_scores[1][1] * 100)  # Format similarity as a string with two decimal places and a percentage sign\n",
        "  else:\n",
        "    paper_indices = int(sim_scores[0][0])\n",
        "    similarity = \"{:.2f}%\".format(sim_scores[0][1] * 100)  # Format similarity as a string with two decimal places and a percentage sign\n",
        "\n",
        "  title = train_df['title'].iloc[paper_indices]\n",
        "  categories = train_df['categories'].iloc[paper_indices]\n",
        "  abstract = train_df['abstract'].iloc[paper_indices]\n",
        "  return title, categories, abstract, similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH6lpgkLGa0b",
        "outputId": "e502c2ce-84ab-453f-bb57-1af3c4f3ee1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('\"Forget time\"',\n",
              " 'gr-qc',\n",
              " \"Following line research have developed several years , argue best strategy understanding quantum gravity is build picture physical world notion time plays role . summarize here point view , explaining think fundamental description nature `` forget time '' , be done classical quantum theory . idea is develop formalism that treats dependent independent variables same footing . short , propose interpret mechanics theory relations variables , rather theory evolution variables time .\",\n",
              " '100.00%')"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_recommendations_TFIDF('''\n",
        "Following line research have developed several years , argue best strategy understanding quantum gravity is build picture physical world notion time plays role . summarize here point view , explaining think fundamental description nature `` forget time '' , be done classical quantum theory . idea is develop formalism that treats dependent independent variables same footing . short , propose interpret mechanics theory relations variables , rather theory evolution variables time .\n",
        "''')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bu6C8ZJzYTO2"
      },
      "source": [
        "# Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tluVY09YSnz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def train_doc2vec_model(corpus, vector_size=100, window=5, min_count=1, epochs=100):\n",
        "    # create tagged document object\n",
        "    tagged_data = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(corpus)] # words=word_tokenize(doc.lower())\n",
        "\n",
        "    # initialize doc2vec model\n",
        "    model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, epochs=epochs)\n",
        "\n",
        "    # build vocabulary\n",
        "    model.build_vocab(tagged_data)\n",
        "\n",
        "    # train doc2vec model\n",
        "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "    return model\n",
        "\n",
        "doc2vec_model = train_doc2vec_model(train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C846nG5AYY53"
      },
      "outputs": [],
      "source": [
        "def get_recommendations_Doc2Vec(abstract):\n",
        "    # remove unnecessary words\n",
        "    abstract = tokenize_POS(abstract)\n",
        "    # train the model\n",
        "    model = doc2vec_model\n",
        "\n",
        "    # infer the vector for the given abstract\n",
        "    abstract_vector = model.infer_vector(abstract.split())\n",
        "\n",
        "    # get the most similar abstract\n",
        "    most_similar = model.dv.most_similar([abstract_vector], topn=2)\n",
        "\n",
        "    # Check if the first result is the input abstract\n",
        "    if train_df_doc['abstract'].iloc[int(most_similar[0][0])].split() == abstract.split():\n",
        "        # print('True')\n",
        "        paper_indices = int(most_similar[1][0])\n",
        "        similarity = \"{:.2f}%\".format(most_similar[1][1] * 100)  # Format similarity as a string with two decimal places and a percentage sign\n",
        "    else:\n",
        "        # print('False')\n",
        "        paper_indices = int(most_similar[0][0])\n",
        "        similarity = \"{:.2f}%\".format(most_similar[0][1] * 100)  # Format similarity as a string with two decimal places and a percentage sign\n",
        "\n",
        "    # Retrieve the details of the most similar abstract\n",
        "    title = train_df['title'].iloc[paper_indices]\n",
        "    categories = train_df['categories'].iloc[paper_indices]\n",
        "    abstract = train_df['abstract'].iloc[paper_indices]\n",
        "    return title, categories, abstract, similarity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TVMmKzgRA-v4"
      },
      "source": [
        "# Deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpsJfMfA_zko",
        "outputId": "f5f7cba3-fb98-4391-b670-ac780735ab82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (3.32.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.95.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: gradio-client>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.2.5)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.22.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.4->gradio) (2023.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.4->gradio) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio) (3.12.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (0.17.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (1.26.15)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "cgoGleBcBCgX",
        "outputId": "ed23738f-cdfd-4929-9072-4d2e9cb4c92c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://ebee077e3938e48120.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ebee077e3938e48120.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ebee077e3938e48120.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(paper):\n",
        "    title_tfidf, categories_tfidf, abstract_tfidf, similarity_tfidf = get_recommendations_TFIDF(paper)\n",
        "    title_doc2vec, categories_doc2vec, abstract_doc2vec, similarity_doc2vec = get_recommendations_Doc2Vec(paper)\n",
        "    return title_tfidf, categories_tfidf, abstract_tfidf, title_doc2vec, categories_doc2vec, abstract_doc2vec\n",
        "\n",
        "title = '''SimSearch:\\n\n",
        "A Similarity Search Tool for Research Paper Abstracts using NLP'''\n",
        "\n",
        "demo = gr.Interface(\n",
        "    title = title,\n",
        "    fn = greet,\n",
        "    inputs=gr.Textbox(placeholder='Abstract Here'),\n",
        "    outputs=[gr.outputs.Textbox(label='''TFIDF Title'''),\n",
        "             gr.outputs.Textbox(label='TFIDF Categories'),\n",
        "             gr.outputs.Textbox(label='TFIDF Abstract'),\n",
        "            #  gr.outputs.Textbox(label='Similarity')\n",
        "             gr.outputs.Textbox(label='''Doc2Vec Title'''),\n",
        "             gr.outputs.Textbox(label='Doc2Vec Categories'),\n",
        "             gr.outputs.Textbox(label='Doc2Vec Abstract'),\n",
        "             ],\n",
        "    examples = [\n",
        "        # ['''In this paper we consider permutations of sequences of partitions, obtaining\\na result which parallels von Neumann's theorem on permutations of dense\\nsequences and uniformly distributed sequences of points.\\n'''],\n",
        "                ['''A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron, and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC, showing that enhanced sensitivity to the signal can be obtained with judicious selection of events.'''],\n",
        "                ['''Most physical experiments are usually described repeated measurements random variables . experimental data registered on-line computers form time series outcomes . frequencies different outcomes are compared probabilities provided algorithms quantum theory ( QT ) . spite statistical predictions QT claim was made theory provided most complete description data underlying physical phenomena . claim be easily rejected fine structures , averaged out standard statistical descriptive analysis , were found time series experimental data . search structures one has use more subtle statistical tools which were developed study time series produced various stochastic processes . talk review tools . example show standard descriptive statistical analysis data is unable reveal fine structure simulated sample AR ( 2 ) stochastic process . emphasize once again violation Bell inequalities gives information completeness non locality QT . appropriate way test completeness quantum theory is search fine structures time series experimental data means purity tests studying autocorrelation partial autocorrelation functions .''']]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
